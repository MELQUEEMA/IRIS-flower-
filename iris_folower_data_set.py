# -*- coding: utf-8 -*-
"""Iris-folower data set.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wXrRdFBPpyP9erw9QNPqDoGxF-0A6fZL
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error 
from sklearn import metrics

df = pd.read_csv('/content/Iris.csv')

df.head()

df.shape

df.isnull().sum()

df.info()

df['Species'].value_counts

df.head()

def plot_correlation_map( df ):

    corr = df.corr()

    s , ax = plt.subplots( figsize =( 15 , 15 ) )

    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )

    s = sns.heatmap(

        corr, 

        cmap = cmap,

        square=True, 

        cbar_kws={ 'shrink' : .9 }, 

        ax=ax, 

        annot = True, 

        annot_kws = { 'fontsize' : 12 }

        )

plot_correlation_map(df)

df.columns

setosa = df[df['Species'] == 'Iris-setosa'].value_counts().sum()

virginica = df[df['Species'] == 'Iris-virginica'].value_counts().sum()

plt.figure(figsize=(6,6));
explode = [0.00,0.0,0]
colors = ['#e796ff','#5a4fcf','#4d0063']
labels = ['Iris-setosa','Iris-virginica','Iris-versicolor']
df['Species'].value_counts().plot(kind='pie',labels=labels,explode = explode,startangle=80, radius=1.2,colors=colors,autopct='%.2f%%');

df['Species'] = df['Species'].map({'Iris-setosa':0 , 'Iris-virginica':1,'Iris-versicolor':2})

df.head()

df.describe()

"""#lets try something like logistic reg. first : """

from sklearn.linear_model import LogisticRegression

x_features = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]
target = df['Species']
x_train, x_test, y_train, y_test = train_test_split(x_features,target,random_state=3)

"""when using logistic reg its better to standarize the data """

scale = StandardScaler()
scale.fit(x_train)
x_train = scale.transform(x_train)
x_test = scale.transform(x_test)

model = LogisticRegression()

model.fit(x_train,y_train);

predicted = model.predict(x_test)

print('MSE = ',mean_squared_error(y_test,predicted)*100,'%')
print("R squared = ", metrics.r2_score(y_test,predicted)*100,'%')

score = model.score(x_test,y_test)

print('score = ',score*100,'%' )

plt.figure(figsize=(12,9))
confusion_matrix = pd.crosstab(y_test, predicted, rownames=['Actual'], colnames=['Predicted']);
sns.heatmap(confusion_matrix, annot=True,fmt='.0f');

"""#Now lets try KNN"""

x_features = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]
target = df['Species']
x_train, x_test, y_train, y_test = train_test_split(x_features,target,random_state=3,test_size=0.3)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

x_train,x_test,y_train,y_test = train_test_split(x_features,target,random_state = 3)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train,y_train)

y_predicted = knn.predict(x_test)

print('MSE = ',mean_squared_error(y_test,y_predicted))

print(round(accuracy_score(y_test,y_predicted)*100))

n_neighbors=5
scores=[]
for k in range(1,5):
    knn=KNeighborsClassifier(n_neighbors-k)
    knn.fit(x_train,y_train)
    y_pred=knn.predict(x_test)
    print('Accuracy for k=',k,'is:',round(accuracy_score(y_pred,y_test),2))
    scores.append(round(accuracy_score(y_pred,y_test),2))

